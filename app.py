import streamlit as st
import pandas as pd
import requests
from io import StringIO

# è¨­å®šç¶²é è³‡è¨Š
st.set_page_config(page_title="å¯Œé‚¦ç”¢éšª | è²¡å ±åŠ©ç†", page_icon="ğŸ›¡ï¸", layout="wide")

# --- 1. æŠ“å–è‚¡ç¥¨æ¸…å–® (å¼·åŒ–é›²ç«¯ç©©å®šåº¦) ---
@st.cache_data(ttl=3600)
def get_stock_list():
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'}
    urls = ["https://isin.twse.com.tw/isin/C_public.jsp?strMode=2", 
            "https://isin.twse.com.tw/isin/C_public.jsp?strMode=4"]
    stock_dict = {}
    
    for url in urls:
        try:
            res = requests.get(url, headers=headers, timeout=10)
            res.encoding = 'big5' # è­‰äº¤æ‰€æ¸…å–®å›ºå®šä½¿ç”¨ big5
            df = pd.read_html(StringIO(res.text))[0]
            df.columns = df.iloc[0]
            df = df.iloc[1:]
            for item in df['æœ‰åƒ¹è­‰åˆ¸ä»£è™ŸåŠåç¨±'].dropna():
                if 'ã€€' in item:
                    code, name = item.split('ã€€')
                    if len(code) == 4:
                        stock_dict[f"{code} {name}"] = code
        except Exception:
            continue
    return stock_dict

# --- 2. çˆ¬å–è²¡å ±æ•¸æ“š ---
def fetch_mops_data(stock_id, year, season, report_type):
    api_map = {"ç¶œåˆæç›Šè¡¨": "ajax_t164sb04", "è³‡ç”¢è² å‚µè¡¨": "ajax_t164sb03"}
    url = f'https://mops.twse.com.tw/mops/web/{api_map[report_type]}'
    payload = {
        'step': '1', 'firstin': '1', 'off': '1', 'queryName': 'co_id',
        'inpuType': 'co_id', 'TYPEK': 'all', 'isnew': 'false',
        'co_id': stock_id, 'year': str(year), 'season': str(season)
    }
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'}

    try:
        res = requests.post(url, data=payload, headers=headers, timeout=15)
        res.encoding = 'utf-8'
        dfs = pd.read_html(StringIO(res.text))
        for df in dfs:
            if 'æœƒè¨ˆé …ç›®' in df.columns.get_level_values(0):
                df.columns = df.columns.get_level_values(0)
                return df[['æœƒè¨ˆé …ç›®', 'é‡‘é¡']].dropna()
        return None
    except:
        return None

# --- 3. UI ä»‹é¢ ---
st.title("ğŸ›¡ï¸ å¯Œé‚¦ç”¢éšª - ä¼æ¥­è²¡å ±åˆ†æåŠ©æ‰‹")
st.info("è¼”åŠ©åŒä»æ ¸ä¿é¢¨éšªè©•ä¼°ï¼Œæ•¸æ“šä¾†æºï¼šå…¬é–‹è³‡è¨Šè§€æ¸¬ç«™ã€‚")

st.sidebar.title("ğŸ” æŸ¥è©¢è¨­å®š")
all_stocks = get_stock_list()

# è§£æ±º KeyError: None èˆ‡ No results çš„é˜²è­·é€»è¾‘
target_id = None
if not all_stocks:
    st.sidebar.error("âš ï¸ ç„¡æ³•ç²å–è‚¡ç¥¨æ¸…å–®ï¼Œè«‹æ‰‹å‹•è¼¸å…¥ä»£ç¢¼")
    target_id = st.sidebar.text_input("è¼¸å…¥è‚¡ç¥¨ä»£ç¢¼ (ä¾‹: 2330)")
else:
    # å¢åŠ ä¸€å€‹ç©ºé¸é …ä½œç‚ºé è¨­ï¼Œé¿å…å•Ÿå‹•æ™‚å ±éŒ¯
    options = ["--- è«‹é¸æ“‡æˆ–è¼¸å…¥å…¬å¸ ---"] + list(all_stocks.keys())
    selected_stock = st.sidebar.selectbox("å…¬å¸åç¨±/ä»£ç¢¼", options=options, index=0)
    
    if selected_stock != "--- è«‹é¸æ“‡æˆ–è¼¸å…¥å…¬å¸ ---":
        target_id = all_stocks.get(selected_stock)

report_type = st.sidebar.radio("å ±è¡¨é¡å‹", ["ç¶œåˆæç›Šè¡¨", "è³‡ç”¢è² å‚µè¡¨"])
year = st.sidebar.selectbox("å¹´ä»½ (æ°‘åœ‹)", ["113", "112", "111", "110"])
season = st.sidebar.selectbox("å­£åº¦", ["01", "02", "03", "04"], index=2)

if st.sidebar.button("ğŸš€ åŸ·è¡Œçˆ¬å–") and target_id:
    with st.spinner('æ•¸æ“šèª¿é–±ä¸­...'):
        df_result = fetch_mops_data(target_id, year, season, report_type)
        if df_result is not None:
            st.success(f"âœ… æŸ¥è©¢æˆåŠŸï¼({target_id})")
            st.dataframe(df_result, use_container_width=True, height=500)
            csv = df_result.to_csv(index=False).encode('utf-8-sig')
            st.download_button("ğŸ“¥ ä¸‹è¼‰å ±è¡¨ (CSV)", csv, f"{target_id}_report.csv")
        else:
            st.error("âŒ æŸ¥ç„¡è³‡æ–™ï¼Œå¯èƒ½è©²å­£å ±å°šæœªä¸Šå‚³ã€‚")
elif not target_id:
    st.warning("ğŸ‘ˆ è«‹å…ˆé¸æ“‡ä¸€å®¶å…¬å¸ä»¥é–‹å§‹æŸ¥è©¢ã€‚")
